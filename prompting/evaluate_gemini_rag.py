#!/usr/bin/env python3

# -------------------------
# INTRODUCTION -> some utils informations about the Python script
# -------------------------


"""
- MODALITÀ:


- PRE-REQUISITI (comandi da eseguire da riga di comando):

    export GOOGLE_API_KEY=CHIAVE API    -> comando per esportare in locale la chiave per eseguire API Gemini (da rifare ogni volta che si chiude il terminale)
    source .env/bin/activate            -> comando per attivare enviroment virtuale
    pip install chromedb                -> comando per scaricare 
    pip install sentence-transformers   -> comando per scaricare 

- COMANDO PER ESECUZIONE (ATTENZIONE -> è necessario eseguire la prima riga di pre-requisiti ogni volta che si chiude il terminale):
    
    - Prova
    
        python3 prompting/evaluate_gemini_rag.py --sessions output/cowrie_TEST.jsonl --index-file output/cowrie_TRAIN.jsonl --k 5 --rag-k 3 --context-len 5 --n 10

    - Intero dataset  

        python3 prompting/evaluate_gemini_rag.py --sessions /media/matteo/T9/outputMerge/cowrie_TEST.jsonl --index-file /media/matteo/T9/outputMerge/cowrie_TRAIN.jsonl --persist-dir /media/matteo/T9/chroma_storage --output output/prova/gemini_rag_results_n10_ctx5_k5.jsonl  --k 5 --rag-k 3 --context-len 5 --n 10

"""

# -------------------------
# IMPORT SECTION -> imports necessary for the Python script
# -------------------------

from __future__ import annotations
import argparse
import os
import sys
import core_rag
from google.genai.types import HarmCategory, HarmBlockThreshold
from google.genai import Client

# =============================================================================
# GEMINI CALLER SECTION -> The function sends a prompt to a model managed by Ollama via an HTTP POST request and returns the response generated by the model.
# =============================================================================

api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    sys.exit("ERRORE CRITICO: La variabile d'ambiente GOOGLE_API_KEY non è impostata.")

client_gemini = Client(api_key=api_key)

def query_gemini(prompt: str, model_name: str, temp: float = 0.0) -> str:

    try:
        #Visto che stiamo simulando degli attacchi, è necessario disattivare i blocchi di sicurezza
        safety_config = [
            {"category": HarmCategory.HARM_CATEGORY_HATE_SPEECH, "threshold": HarmBlockThreshold.BLOCK_NONE},
            {"category": HarmCategory.HARM_CATEGORY_HARASSMENT, "threshold": HarmBlockThreshold.BLOCK_NONE},
            {"category": HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, "threshold": HarmBlockThreshold.BLOCK_NONE},
            {"category": HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, "threshold": HarmBlockThreshold.BLOCK_NONE},
        ]

        response = client_gemini.models.generate_content(
            model=model_name,
            contents=prompt,
            config={
                "temperature": temp,
                "top_p": 0.1,
                "max_output_tokens": 1024,
                "safety_settings": safety_config # APPLICHIAMO I FILTRI PERMISSIVI
            }
        )
        
        # Controllo difensivo: se il modello restituisce None o non ha testo
        if not response or not response.text: return "" 
        else: return response.text

    except Exception as exc:
        err_str = str(exc)
        # Gestione dell'errore in caso di error 404
        if "404" in err_str: 
            print(f"\n[ERRORE FATALE] Modello '{model_name}' non trovato.")
            sys.exit(1)
        # Restituzione di una stringa vuota per non rompere il loop
        return ""

# =============================================================================
# SECTION MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Evaluate Gemini with RAG Vector Search")
    parser.add_argument("--sessions", required=True, help="File JSONL con le sessioni di test")
    parser.add_argument("--persist-dir", default="./chroma_storage", help="Cartella contenente db vettoriale")
    parser.add_argument("--index-file", help="File JSONL per DB vettoriale (se diverso da sessions)")
    parser.add_argument("--output", default=None, help="Nome file output")
    parser.add_argument("--model", default="gemini-flash-latest", help="Nome modello (es. gemini-1.5-pro-latest, gemini-pro)")  # modello spesso più stabile
    parser.add_argument("--k", type=int, default=5, help="Numero di predizioni")
    parser.add_argument("--rag-k", type=int, default=3, help="Esempi storici da recuperare")
    parser.add_argument("--context-len", type=int, default=5, help="Lunghezza contesto")
    parser.add_argument("--n", type=int, default=0, help="Max test (0=tutti)")

    args = parser.parse_args()

    if args.output is None: args.output = f"output/prova/gemini_rag_results_n{args.n}_ctx{args.context_len}_k{args.k}.jsonl"

    # Modifico il nome della cartella di contenimento dei vettori -> db fortemente influenzato da context_len
    args.persist_dir = f"{args.persist_dir}_ctx{args.context_len}"

    core_rag.prediction_evaluation(args, query_model=query_gemini)

if __name__ == "__main__":
    main()