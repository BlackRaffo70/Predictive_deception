#!/usr/bin/env python3

# -------------------------
# INTRODUCTION -> some utils informations about the Python script
# -------------------------

"""
- MODALITÀ:
    Il file contiene la funzione query_ollama() per mandare il prompt a LLM ollama (è possibile scegliere il modello)
    Tutte le funzionalità di effettivo prompting sono contenute all'interno del file core_topk.py in quanto 
    in comune con lo script evaluate_GEMINI_topk.py. I risultati della valutazione della prediction vengono
    salvati nel file output/ollama_topk_result.jsonl

- PRE-REQUISITI (comandi da eseguire da riga di comando):

    source .venv/bin/activate           -> comando per attivare enviroment virtuale
    pip install requests tqdm           -> comando per installare libreria requests (per effettuare richieste HTTP) e tqdm (mostrare barre di avanzamento per monitorare l'esecuzione di operazioni)      

    - Comando per installare il modello di ollama:
    ollama pull NOME_MODELLO

- COMANDO PER ESECUZIONE (che varia della funzione e del modello utilizzato):
    
    - Per quanto riguarda prompting con contesto, a seconda del modello:
        - codellama

        python prompting/evaluate_ollama_topk.py --sessions output/cowrie_ALL_CLEAN.jsonl --model codellama --k 5 --context-len 3 --output output/ollama_topk_results.jsonl --n 10

        - llama3:8b

        python prompting/evaluate_ollama_topk.py --sessions output/cowrie_ALL_CLEAN.jsonl --model llama3:8b --k 5 --context-len 3 --output output/ollama_topk_results.jsonl --n 10

    - Per quanto riguarda prompting SENZA contesto:
    
        python prompting/evaluate_ollama_topk.py --single-cmd "cat /proc/cpuinfo | grep name | wc -l" --model gemma:2b --k 5 --output output/single_results.jsonl

    dove le varie flag sono:
    - sessions = (solo CON contesto) flag attraverso cui si passa il file contente le sessioni di attacco (che al loro interno presentano i comandi su cui bisogna eseguire la prediction)
    - single-cmd = (solo SENZA contesto) flag attraverso cui si passa il comando di cui è necessario predirne il successivo
    - model = modello LLM utilizzato
    - k = numero di comandi generati per la prediction
    - out = file che viene popolato con i risultati della prediction eseguita
    - n = (solo CON contesto) numero di comandi 
    - context-len = (solo CON contesto) numero di comandi precedenti al comando di cui bisogna prevederne il successivo (forniscono il contesto di attacco per LLM)
"""

# -------------------------
# IMPORT SECTION -> imports necessary for the Python script
# -------------------------

from __future__ import annotations
import argparse
import requests
import core_topk

# -------------------------
# OLLAMA CALLER SECTION -> The function sends a prompt to a model managed by Ollama via an HTTP POST request and returns the response generated by the model.
# -------------------------

def query_ollama(prompt: str, model: str, url: str, temp: float=0.0, timeout: int=120) -> str:
    payload = {"model": model, "prompt": prompt, "temperature": temp, "stream": False, "options": {"top_p": 0.1}}
    try:
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        text = response.json().get("response", "")
        return text.strip()
    
    except Exception as exc:
        print(f"[OLLAMA ERROR] {exc}")
        return ""

# -------------------------
# MAIN SECTION
# -------------------------

def main():
    ap = argparse.ArgumentParser(description="Evaluate Ollama top-K next-command prediction (sessions or single).")
    ap.add_argument("--sessions", help="JSONL sessions file: one JSON per line with fields: session, commands (list)")
    ap.add_argument("--single-cmd", choices=["yes", "no"], default="no", help="Per abilitare la prediction di un solo comando")
    ap.add_argument("--cmd", default=None, help="Single command string to predict next for")
    ap.add_argument("--model", default="gemma:2b", help="Ollama model name")
    ap.add_argument("--ollama-url", default="http://localhost:11434/api/generate")
    ap.add_argument("--output", default=None)
    ap.add_argument("--k", type=int, default=5, help="Top-K candidates")
    ap.add_argument("--context-len", type=int, default=3, help="Context length when using sessions")
    ap.add_argument("--guarateed-ctx", choices=["yes", "no"], default="yes", help="Per la creazione dei task, se il valore è yes, viene garantita la presenta di contesto costituita da context-len comandi")
    ap.add_argument("--n", type=int, default=0, help="Max steps to evaluate (0 = all)")
    ap.add_argument("--temp", type=float, default=0.15)
    ap.add_argument("--sleep", type=float, default=0.05)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()
    if args.output is None:
        args.output = f"output/topk/gemini_topk_results_n{args.n}_ctx{args.context-len}_k{args.k}.jsonl"
    core_topk.prediction_evaluation(args, "ollama", query_model=query_ollama)


if __name__ == "__main__":
    main()