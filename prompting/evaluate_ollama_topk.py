#!/usr/bin/env python3

# -------------------------
# INTRODUCTION -> some utils informations about the Python script
# -------------------------

"""
- MODALITÀ:

    - sessioni: valuta su file JSONL con sessioni (sliding window di predizioni)
    - single: prende un singolo comando (--single-cmd) o file di comandi (--single-file)
    Per ogni predizione richiede top-K candidate a Ollama e:
    - stampa Expected (se disponibile) e poi i K candidate uno per riga
    - confronta permissivamente solo command name + path (ignora flag)
    - salva risultati in JSONL e summary.json

- PRE-REQUISITI (comandi da eseguire da riga di comando):

    source .venv/bin/activate           -> comando per attivare enviroment virtuale
    pip install requests tqdm           -> comando per installare libreria requests (per effettuare richieste HTTP) e tqdm (mostrare barre di avanzamento per monitorare l'esecuzione di operazioni)      

    - Comando per installare il modello di ollama:
    ollama pull NOME_MODELLO

- COMANDO PER ESECUZIONE (che varia della funzione e del modello utilizzato):
    
    - Per quanto riguarda prompting con contesto, a seconda del modello:
        - codellama

        python prompting/evaluate_ollama_topk.py --sessions output/cowrie_ALL_CLEAN.jsonl --model codellama --k 5 --context-len 3 --out output/ollama_topk_results.jsonl --n 10

        - llama3:8b

        python prompting/evaluate_ollama_topk.py --sessions output/cowrie_ALL_CLEAN.jsonl --model llama3:8b --k 5 --context-len 3 --out output/ollama_topk_results.jsonl --n 10

    - Per quanto riguarda prompting SENZA contesto:
    
        python prompting/evaluate_ollama_topk.py --single-cmd "cat /proc/cpuinfo | grep name | wc -l" --model gemma:2b --k 5 --out output/single_results.jsonl

    dove le varie flag sono:
    - sessions = (solo CON contesto) flag attraverso cui si passa il file contente le sessioni di attacco (che al loro interno presentano i comandi su cui bisogna eseguire la prediction)
    - single-cmd = (solo SENZA contesto) flag attraverso cui si passa il comando di cui è necessario predirne il successivo
    - model = modello LLM utilizzato
    - k = numero di comandi generati per la prediction
    - out = file che viene popolato con i risultati della prediction eseguita
    - n = (solo CON contesto) numero di comandi 
    - context-len = (solo CON contesto) numero di comandi precedenti al comando di cui bisogna prevederne il successivo (forniscono il contesto di attacco per LLM)
"""

# -------------------------
# IMPORT SECTION -> imports necessary for the Python script
# -------------------------

from __future__ import annotations
import argparse
import requests
import core_topk

# -------------------------
# OLLAMA CALLER SECTION -> The function sends a prompt to a model managed by Ollama via an HTTP POST request and returns the response generated by the model.
# -------------------------

def query_ollama(prompt: str, model: str, url: str, temp: float=0.2, timeout: int=90) -> str:
    payload = {"model": model, "prompt": prompt, "temperature": temp, "stream": False}
    try:
        r = requests.post(url, json=payload, timeout=timeout)
    except requests.exceptions.RequestException as e:
        raise ConnectionError(f"Ollama connection error: {e}")
    if r.status_code != 200:
        raise RuntimeError(f"Ollama returned HTTP {r.status_code}: {r.text[:400]}")
    try:
        data = r.json()
    except Exception:
        return r.text.strip()
    if isinstance(data, dict):
        for k in ("response","output","text"):
            if k in data and isinstance(data[k], str):
                return data[k].strip()
        if "responses" in data and isinstance(data["responses"], list) and data["responses"]:
            r0 = data["responses"][0]
            return r0 if isinstance(r0, str) else str(r0)
    return r.text.strip()

# -------------------------
# MAIN SECTION
# -------------------------
def main():
    ap = argparse.ArgumentParser(description="Evaluate Ollama top-K next-command prediction (sessions or single).")
    ap.add_argument("--sessions", help="JSONL sessions file: one JSON per line with fields: session, commands (list)")
    ap.add_argument("--single-cmd", help="Single command string to predict next for")
    ap.add_argument("--single-file", help="File with commands (one per line), run prediction for each")
    ap.add_argument("--model", default="gemma:2b", help="Ollama model name")
    ap.add_argument("--ollama-url", default="http://localhost:11434/api/generate")
    ap.add_argument("--out", default="output/ollama_topk_results.jsonl")
    ap.add_argument("--k", type=int, default=5, help="Top-K candidates")
    ap.add_argument("--context-len", type=int, default=3, help="Context length when using sessions")
    ap.add_argument("--n", type=int, default=0, help="Max steps to evaluate (0 = all)")
    ap.add_argument("--temp", type=float, default=0.15)
    ap.add_argument("--sleep", type=float, default=0.05)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()
    core_topk.prediction_evaluation(args, "ollama", query_model=query_ollama)


if __name__ == "__main__":
    main()