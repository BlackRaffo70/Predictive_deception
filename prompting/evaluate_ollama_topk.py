#!/usr/bin/env python3

# -------------------------
# INTRODUCTION -> some utils informations about the Python script
# -------------------------

"""
- MODALITÀ:
    Il file contiene la funzione query_ollama() per mandare il prompt a LLM ollama (è possibile scegliere il modello)
    Tutte le funzionalità di effettivo prompting sono contenute all'interno del file core_topk.py in quanto 
    in comune con lo script evaluate_gemini_topk.py. 

- PRE-REQUISITI (comandi da eseguire da riga di comando):

    source .venv/bin/activate           -> comando per attivare enviroment virtuale
    pip install requests tqdm           -> comando per installare libreria requests (per effettuare richieste HTTP) e tqdm (mostrare barre di avanzamento per monitorare l'esecuzione di operazioni)      

    - Comando per installare il modello di ollama:

        ollama pull NOME_MODELLO

- COMANDO PER ESECUZIONE (che varia della funzione e del modello utilizzato):
    
    - Per quanto riguarda prompting SENZA whitelist:

        python3 prompting/evaluate_ollama_topk.py --sessions /media/matteo/T9/outputMerge/cowrie_ALL_CLEAN.jsonl --k 5 --context-len 3 --n 10 --whitelist no

    - Per quanto riguarda prompting CON whitelist:
    
        python3 prompting/evaluate_ollama_topk.py --sessions /media/matteo/T9/outputMerge/cowrie_ALL_CLEAN.jsonl --k 5 --context-len 3 --n 10

    dove le varie flag sono:
    - sessions = file contenente le sessioni di attacco (che al loro interno presentano i comandi su cui bisogna eseguire la prediction)
    - whitelist = flag per specificare se eseguire il prompting che integra l'utilizzo di whitelist
    - output = per specificare il nome del file che contiene le prediction
    - k = numero di comandi generati per la prediction
    - model = per specificare il modello di Ollama
    - ollama-url = per specificare l'url per eseguire prompt Ollama
    - n = numero di predictio da eseguire per test  
    - context-len = numero di comandi precedenti al comando di cui bisogna prevederne il successivo (forniscono il contesto di attacco per LLM)
"""

# -------------------------
# IMPORT SECTION -> imports necessary for the Python script
# -------------------------

from __future__ import annotations
import argparse
import requests
import core_topk

# -------------------------
# OLLAMA CALLER SECTION -> The function sends a prompt to a model managed by Ollama via an HTTP POST request and returns the response generated by the model.
# -------------------------

def query_ollama(prompt: str, model: str, url: str, temp: float=0.0, timeout: int=120) -> str:
    payload = {"model": model, "prompt": prompt, "temperature": temp, "stream": False, "options": {"top_p": 0.1}}
    try:
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        text = response.json().get("response", "")
        return text.strip()
    
    except Exception as exc:
        print(f"[OLLAMA ERROR] {exc}")
        return ""

# -------------------------
# MAIN SECTION
# -------------------------

def main():
    ap = argparse.ArgumentParser(description="Evaluate Ollama top-K next-command prediction (sessions or single).")
    ap.add_argument("--sessions", help="File json contenenti le sessioni: ogni riga deve essere strutturata come: session, commands (list)")
    ap.add_argument("--whitelist", choices=["yes", "no"], default="yes", help="Con opzione attivata, esegue il prompt con whitelist")
    ap.add_argument("--model", default="codellama", help="Nome modello Ollama")
    ap.add_argument("--ollama-url", default="http://localhost:11434/api/generate")
    ap.add_argument("--output", default=None, help="Nome del file dove verranno generati i risultati della prediction")
    ap.add_argument("--k", type=int, default=5, help="Candidati proposti come next command dell'attaccante")
    ap.add_argument("--context-len", type=int, default=5, help="Numero di comandi che rappresentano il contesto di attacco")
    ap.add_argument("--n", type=int, default=0, help="Numero di prediction da eseguire (0 = una prediction per ogni sessione del file di input)")
    
    args = ap.parse_args()
    if args.output is None:
        args.output = f"output/topk/ollama/ollama_topk_results_n{args.n}_ctx{args.context_len}_k{args.k}.jsonl"
    core_topk.prediction_evaluation(args, "ollama", query_model=query_ollama)

if __name__ == "__main__":
    main()