#!/usr/bin/env python3

# -------------------------
# INTRODUCTION -> some utils informations about the Python script
# -------------------------


"""
- MODALITÀ:
    Il file contiene la funzione query_ollama() per mandare il prompt a LLM ollama (è possibile scegliere il modello). 
    Tutte le funzionalità per il supporto all'approccio RAG (Retrieval-Augmented Generation)
    sono contenute all'interno del file core_rag.py in quanto in comune con lo script evaluate_gemini_rag.py. I risultati della 
    valutazione della prediction vengono salvati nel file output/prova/ollama_rag_result_*.jsonl o 
    output/dataset/ollama_rag_result_*.jsonl a seconda del numero di file e parametri utilizzati.

- PRE-REQUISITI (comandi da eseguire da riga di comando):

    source .env/bin/activate            -> comando per attivare enviroment virtuale
    pip install chromedb                -> comando per scaricare 
    pip install sentence-transformers   -> comando per scaricare 

- COMANDO PER ESECUZIONE (ATTENZIONE -> è necessario eseguire la prima riga di pre-requisiti ogni volta che si chiude il terminale):

    - Prova su pochi file del dataset (contenuti nella cartella ./data)

        python3 prompting/evaluate_ollama_rag.py --sessions output/cowrie_TEST.jsonl --index-file output/cowrie_TRAIN.jsonl --k 5 --rag-k 3 --context-len 5 --n 10

    - Intero dataset (contenuto su dispositivo di archiviazione esterna) 

        python3 prompting/evaluate_ollama_rag.py --sessions /media/matteo/T9/outputMerge/cowrie_TEST.jsonl --index-file /media/matteo/T9/outputMerge/cowrie_TRAIN.jsonl --persist-dir /media/matteo/T9/chroma_storage --output output/rag/dataset/ollama_rag_results_n10_ctx5_k5.jsonl --k 5 --rag-k 3 --context-len 5 --n 10

    dove le varie flag sono:
    - sessions = per specificare file jsonl contenente le sessioni per eseguire prediction
    - persist-dir = per specificare cartella contenente DB vettoriale
    - index-file = per specificare file jsonl per indicizzazione del DB vettoriale (se diverso da sessions)
    - output = per specificare nome del file dove verranno generati i risultati della prediction
    - model = per specificare nome modello Ollama
    - ollama-url = url per inviare il prompt al modello ollama in locale
    - k = candidati proposti come next command dell'attaccante
    - rag-k = esempi storici da recuperare nel DB vettoriale
    - context-len = numero di comandi che rappresentano il contesto di attacco
    - n = numero di prediction da eseguire (0 = una prediction per ogni sessione del file di input)

- OSSERVAZIONI:

    L'esecuzione di questo codice può portare a due problemi in particolare:

        - [OLLAMA ERROR] Extra data: line 2 column 1 (char 97) -> risolto attraverso la disattivazione dello stream nella risposta del modello
        - [OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) -> ristretto aumentando il tempo di timeout. Questo problema è dovuto principalmente ad un sovraccarico del PC dovuto al modello utilizzato e alla disattivazione dello streaming che costringe ollama a generare l'intera risposta prima di inviarla
        - Formato del file di output contenente backtip e elenchi numerati che comprettevano la valutazione delle prediction -> problema dovuto al modello codellama, risolto attraverso l'introduzione della funzione clean_ollama_candidate(line: str)
"""

# -------------------------
# IMPORT SECTION -> imports necessary for the Python script
# -------------------------

from __future__ import annotations
import argparse
import requests
import core_rag

# =============================================================================
# OLLAMA CALLER SECTION -> The function sends a prompt to a model managed by Ollama via an HTTP POST request and returns the response generated by the model.
# =============================================================================

def query_ollama(prompt: str, model: str, url: str, temp: float = 0.0, timeout: int=120) -> str:
    payload = {"model": model, "prompt": prompt, "stream": False, "temperature": temp, "options": {"top_p": 0.1}}     
    try:
        response = requests.post(url, json=payload, timeout=timeout)
        response.raise_for_status()
        text = response.json().get("response", "")

        return text.strip()

    except Exception as exc:
        print(f"[OLLAMA ERROR] {exc}")
        return ""

# =============================================================================
# SECTION MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Evaluate Gemini with RAG Vector Search")
    parser.add_argument("--sessions", required=True, help="File jsonl contenente le sessioni per eseguire prediction")
    parser.add_argument("--persist-dir", default="./chroma_storage", help="Cartella contenente DB vettoriale")
    parser.add_argument("--index-file", help="File jsonl per indicizzazione del DB vettoriale (se diverso da sessions)")
    parser.add_argument("--output", default=None, help="Nome del file dove verranno generati i risultati della prediction")
    parser.add_argument("--model", default="codellama", help="Modello Ollama (es. llama3, mistral, codellama)")
    parser.add_argument("--ollama-url", default="http://localhost:11434/api/generate")
    parser.add_argument("--k", type=int, default=5, help="Candidati proposti come next command dell'attaccante")
    parser.add_argument("--rag-k", type=int, default=3, help="Esempi storici da recuperare nel DB vettoriale")
    parser.add_argument("--context-len", type=int, default=5, help="Numero di comandi che rappresentano il contesto di attacco")
    parser.add_argument("--n", type=int, default=0, help="Numero di prediction da eseguire (0 = una prediction per ogni sessione del file di input)")
    
    args = parser.parse_args()
    if args.output is None: args.output = f"output/rag/ollama_rag_results_n{args.n}_ctx{args.context_len}_k{args.k}.jsonl"
    # Modifico il nome della cartella di contenimento dei vettori -> db fortemente influenzato da context_len
    args.persist_dir = f"{args.persist_dir}_ctx{args.context_len}"
    
    core_rag.prediction_evaluation(args, "ollama", query_model=query_ollama)

if __name__ == "__main__":
    main()