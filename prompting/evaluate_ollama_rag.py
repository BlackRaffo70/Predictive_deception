#!/usr/bin/env python3

# -------------------------
# INTRODUCTION -> some utils informations about the Python script
# -------------------------


"""
- MODALITÀ:


- PRE-REQUISITI (comandi da eseguire da riga di comando):

    source .env/bin/activate            -> comando per attivare enviroment virtuale
    pip install chromedb                -> comando per scaricare 
    pip install sentence-transformers   -> comando per scaricare 

- COMANDO PER ESECUZIONE (ATTENZIONE -> è necessario eseguire la prima riga di pre-requisiti ogni volta che si chiude il terminale):

    - Prova

        python3 prompting/evaluate_ollama_rag.py --sessions output/cowrie_TEST.jsonl --index-file output/cowrie_TRAIN.jsonl --k 5 --rag-k 3 --context-len 5 --n 10

    - Intero dataset

        python3 prompting/evaluate_ollama_rag.py --sessions /media/matteo/T9/outputMerge/cowrie_TEST.jsonl --index-file /media/matteo/T9/outputMerge/cowrie_TRAIN.jsonl --persist-dir /media/matteo/T9/DB_vettoriale --k 5 --rag-k 3 --context-len 5 --n 10

- OSSERVAZIONI:

    L'esecuzione di questo codice può portare a due problemi in particolare:

        - [OLLAMA ERROR] Extra data: line 2 column 1 (char 97) -> risolto attraverso la disattivazione dello stream nella risposta del modello
        - [OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) -> ristretto aumentando il tempo di timeout. Questo problema è dovuto principalmente ad un sovraccarico del PC dovuto al modello utilizzato e alla disattivazione dello streaming che costringe ollama a generare l'intera risposta prima di inviarla
        - Formato del file di output contenente backtip e elenchi numerati che comprettevano la valutazione delle prediction -> problema dovuto al modello codellama, risolto attraverso l'introduzione della funzione clean_ollama_candidate(line: str)
"""

# -------------------------
# IMPORT SECTION -> imports necessary for the Python script
# -------------------------

from __future__ import annotations
import argparse
import requests
import core_rag

# =============================================================================
# OLLAMA CALLER SECTION -> The function sends a prompt to a model managed by Ollama via an HTTP POST request and returns the response generated by the model.
# =============================================================================

def query_ollama(prompt: str, model_name: str, temp: float = 0.0) -> str:

    try:
        payload = {"model": model_name, "prompt": prompt, "stream": False, "options": {"temperature": temp,"top_p": 0.1}}
        response = requests.post("http://localhost:11434/api/generate", json=payload, timeout=120)
        response.raise_for_status()
        text = response.json.get("response", "")

        return text.strip()

    except Exception as exc:
        print(f"[OLLAMA ERROR] {exc}")
        return ""

# =============================================================================
# SECTION MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Evaluate Gemini with RAG Vector Search")
    parser.add_argument("--sessions", required=True, help="File JSONL con le sessioni di test")
    parser.add_argument("--persist-dir", default="./chroma_storage", help="Cartella contenente db vettoriale")
    parser.add_argument("--index-file", help="File JSONL per DB vettoriale (se diverso da sessions)")
    parser.add_argument("--output", default=None, help="Nome file output")
    parser.add_argument("--model", default="codellama", help="Modello Ollama (es. llama3, mistral, codellama)")
    parser.add_argument("--k", type=int, default=5, help="Numero di predizioni")
    parser.add_argument("--rag-k", type=int, default=3, help="Esempi storici da recuperare")
    parser.add_argument("--context-len", type=int, default=5, help="Lunghezza contesto")
    parser.add_argument("--n", type=int, default=0, help="Max test (0=tutti)")
    
    args = parser.parse_args()
    if args.output is None:
        args.output = f"output/ollama_rag_results_n{args.n}_ctx{args.context_len}_k{args.k}.jsonl"

    core_rag.prediction_evaluation(args, query_model=query_ollama)

if __name__ == "__main__":
    main()