<img width="1024" height="233" alt="image" src="https://github.com/user-attachments/assets/e210dcce-57f2-4470-a895-780896dbe45f" />

# ğŸ¯ Predictive Deception â€” LLM-based Command Anticipation in SSH Honeypots

---

## ğŸ¯ Obiettivo del progetto â€” *Predictive Deception per Honeypot*

Gli honeypot tradizionali osservano e registrano ciÃ² che lâ€™attaccante fa **solo dopo** lâ€™esecuzione di un comando.  
Questo progetto introduce un nuovo paradigma: sfruttare un **LLM** per trasformare lâ€™honeypot da sistema passivo a **sistema predittivo e adattivo**.

---

## ğŸš€ Idea chiave  
Un modello di linguaggio (CodeLlama, Llama 3, Gemini, Mistral) analizza in tempo reale la sequenza dei comandi dellâ€™attaccante e **predice il prossimo comando** con elevata accuratezza â€” *prima che venga digitato*.

Questa predizione permette al sistema di costruire sul filesystem una *realtÃ  manipolata* che lâ€™attaccante non puÃ² distinguere da quella autentica.

---

## ğŸª¤ Core Concept: Predictive Deception

La **deception** in questo progetto non Ã¨ statica, ma *dinamica e reattiva*.  
Si basa su tre principi fondamentali:

### 1ï¸âƒ£ *Preparazione anticipata*  
Prima che lâ€™attaccante esegua il comando previsto, il sistema crea:

- file fake ma plausibili  
- directory esca  
- script o configurazioni manipolate  
- log fittizi  
- output alterati

Il tutto generato automaticamente dal modello LLM, a seconda del comando predetto.

Esempio:  
se il modello predice `cat /etc/passwd`, il sistema puÃ² generare una versione *decoy*, coerente ma falsificata.

---

### 2ï¸âƒ£ *Branching e pruning*  
Per ogni comando vengono predetti **5 possibili next-steps**.

Per ciascuno viene generata una *branch* di deception:

- branch A â†’ file X  
- branch B â†’ directory Y  
- branch C â†’ canary token Z  
- â€¦

Quando lâ€™attaccante esegue realmente un comando, il sistema:

- mantiene **solo la branch corretta**  
- elimina le altre 4 con cleanup automatico  
- conserva coerenza assoluta nel filesystem

Questo crea la sensazione di un sistema vivo e coerente, impossibile da sgamare.

---

### 3ï¸âƒ£ *Deception adattiva basata su RAG*  
L'LLM non predice "a caso".  
Combina:

- history della sessione  
- sequenze di attacchi precedenti conservate via **ChromaDB RAG**

In questo modo la deception diventa *personalizzata*:

- se lâ€™attaccante mostra pattern simili a botnet â†’ deception tecnica  
- se mostra pattern umani â†’ deception narrativa e coerente  
- se usa tool come Mirai, Tsunami, zmap â†’ deception su file system e servizi

---

## ğŸ” PerchÃ© Ã¨ rivoluzionario

- ğŸª¤ **Deception mirata e contestuale**  
  Non Ã¨ la solita deception statica: il sistema modifica lâ€™ambiente *in tempo reale* in base al comportamento.

- ğŸ¯ **Trigger nascosti intelligenti**  
  Canary tokens, honey-credentials e file monitoring attivati solo quando utili.

- ğŸ§  **Ingaggio dellâ€™attaccante aumentato**  
  Lâ€™ambiente sembra perfettamente reale, con struttura coerente e reattiva.

- ğŸ“¡ **Threat intelligence potenziata**  
  La correlazione via RAG tra comandi vecchi e nuovi permette un profiling comportamentale avanzato.

- ğŸ› **Riduzione dei falsi positivi**  
  La predizione contestuale evita di generare deception non rilevanti.

---

## ğŸ§© In sintesi  
Il progetto trasforma lâ€™honeypot in un sistema **proattivo**, capace non solo di osservare ma di:

- **anticipare** lâ€™attaccante  
- **modellare** lâ€™ambiente in base al suo comportamento  
- **manipolare** la percezione dellâ€™host  
- **studiare** tecniche emergenti tramite dataset predittivi

Si passa cosÃ¬ da un honeypot statico a un sistema **intelligente, adattivo e realmente interattivo**, in grado di raccogliere informazioni impossibili da ottenere con soluzioni tradizionali.

---

## ğŸ“¦ Requirements

Il progetto utilizza LLM, RAG e dataset da honeypot (Cowrie) per analisi predittiva, deception adattiva e fine-tuning dei modelli.  
Di seguito lâ€™elenco completo e organizzato delle dipendenze necessarie.

---

## ğŸ”§ Core Dependencies
Librerie principali per gestione ambiente, logging, preprocessing e networking.

- `python-dotenv` â€” gestione variabili dâ€™ambiente
- `tqdm` â€” progress bar e logging
- `requests` â€” API e download dataset
- `jsonlines` â€” lettura/scrittura JSONL
- `pandas` â€” preprocess e analisi dataset

---

## ğŸ§  RAG & Embeddings  
Moduli necessari per creare e interrogare il database vettoriale basato su ChromaDB.

- `chromadb`
- `sentence-transformers`  
  (utilizzato per MiniLM-L6-v2 nei retrieval)

---

## ğŸ¤– LLM APIs (Gemini / OpenAI / HF)
Integrazione con i principali modelli LLM utilizzati nel progetto per predizione e generazione degli artefatti di deception.

- `openai`
- `google-genai`
- `transformers`
- `tokenizers`
- `safetensors`

---

## ğŸ§ª Fine-Tuning (CodeLlama / LoRA / PEFT)
Dipendenze necessarie per addestramento leggero (LoRA) su dataset Cowrie + sequenze attaccante.

- `torch`
- `accelerate`
- `datasets`
- `peft`
- `bitsandbytes`  
  (quantizzazione 4/8-bit per GPU poco potenti)

---

## ğŸ“Š Machine Learning Utilities
Strumenti usati per normalizzazione, feature engineering, clustering comportamentale.

- `scikit-learn`
- `numpy`

---

## ğŸ” Nota
Tutte le librerie sono compatibili con Python **3.10+** e con ambienti virtuali standard (`venv`/`conda`).  
Per lâ€™ecosistema LLM Ã¨ consigliata una GPU NVIDIA con supporto CUDA, ma il sistema funziona anche in CPU per test, predizione e RAG.

## ğŸ“ Struttura del repository

```bash
Predictive_deception/
â”‚
â”œâ”€â”€ chroma_storage/                     # Database vettoriale ChromaDB
â”‚   â”œâ”€â”€ chroma.sqlite3
â”‚   â””â”€â”€ DB_checkpoint.txt
â”‚
â”œâ”€â”€ deception/                          # Motore di deception + defender runtime
â”‚   â”œâ”€â”€ scenarios/
â”‚   â”œâ”€â”€ brain.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ defender.py
â”‚   â”œâ”€â”€ host.key
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ session_handler.py
â”‚   â””â”€â”€ ssh_server.py
â”‚
â”œâ”€â”€ Honeypot/                           # Ambiente honeypot (Vagrant + Ansible)
â”‚   â”œâ”€â”€ Vagrantfile
â”‚   â”œâ”€â”€ playbook.yml
â”‚   â”œâ”€â”€ readme.txt
â”‚   â””â”€â”€ roles/
â”‚       â”œâ”€â”€ db_vettoriale/
â”‚       â”‚   â””â”€â”€ tasks/
â”‚       â”œâ”€â”€ defender/
â”‚       â”‚   â”œâ”€â”€ files/
â”‚       â”‚   â”‚   â””â”€â”€ defender2.py
â”‚       â”‚   â”œâ”€â”€ tasks/
â”‚       â”‚   â””â”€â”€ vars/
â”‚       â”œâ”€â”€ env_python/
â”‚       â”‚   â”œâ”€â”€ tasks/
â”‚       â”‚   â””â”€â”€ vars/
â”‚       â””â”€â”€ fakeshell_v2/
â”‚           â”œâ”€â”€ files/
â”‚           â”‚   â”œâ”€â”€ fakeshell.py
â”‚           â”‚   â””â”€â”€ fakeshell_easy.py
â”‚           â”œâ”€â”€ handlers/
â”‚           â””â”€â”€ tasks/
â”‚
â”œâ”€â”€ inspectDataset/                     # Analisi e pulizia dataset Cowrie
â”‚   â”œâ”€â”€ analyze_and_clean.py
â”‚   â”œâ”€â”€ download_zenodo.py
â”‚   â””â”€â”€ merge_cowrie_datasets.py
â”‚
â”œâ”€â”€ prompting/                          # Motore predittivo LLM
â”‚   â”œâ”€â”€ core_rag.py
â”‚   â”œâ”€â”€ core_topk.py
â”‚   â”œâ”€â”€ evaluate_gemini_rag.py
â”‚   â”œâ”€â”€ evaluate_gemini_topk.py
â”‚   â”œâ”€â”€ evaluate_ollama_rag.py
â”‚   â”œâ”€â”€ evaluate_ollama_topk.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```

â¸»
---
## ğŸ§­ Workflow del progetto (script principali)

| Step | Script / File                                                | Input                               | Output                                      | Descrizione |
|------|--------------------------------------------------------------|-------------------------------------|---------------------------------------------|-------------|
| 1ï¸âƒ£  | `inspectDataset/download_zenodo.py`                          | â€”                                   | `data/*.tar.gz`, `data/*.json`              | Scarica automaticamente i dataset Cowrie da Zenodo e li salva nella cartella dati locale. |
| 2ï¸âƒ£  | `inspectDataset/analyze_and_clean.py`                        | `data/*.json`                       | `*_RAW.jsonl`, `*_CLEAN.jsonl`, statistiche | Analizza i log Cowrie, normalizza i comandi, pulisce rumore/duplicati e produce versioni RAW/CLEAN in JSONL. |
| 3ï¸âƒ£  | `inspectDataset/merge_cowrie_datasets.py`                    | `*_CLEAN.jsonl`                     | `cowrie_ALL_*.jsonl`, `cowrie_TRAIN/TEST`   | Unisce piÃ¹ file puliti, crea un dataset unico e lo split train/test per gli esperimenti. |
| 4ï¸âƒ£  | `prompting/core_rag.py`                                      | `cowrie_TRAIN.jsonl`, `chroma_storage/` | `chroma_storage/*`                          | Costruisce e interroga il database vettoriale ChromaDB (embedding + retrieval) per il RAG. |
| 5ï¸âƒ£  | `prompting/core_topk.py`                                     | Sessioni JSONL                      | â€”                                           | Motore generico di predizione Top-k (senza RAG), riusato dai vari script di valutazione. |
| 6ï¸âƒ£  | `prompting/evaluate_gemini_topk.py`                          | `cowrie_TEST.jsonl`                | `output/gemini_topk_results.jsonl`          | Valuta lâ€™API Gemini in modalitÃ  Top-k, misurando Top-1/Top-5 sulle sessioni di test. |
| 7ï¸âƒ£  | `prompting/evaluate_gemini_rag.py`                           | `cowrie_TEST.jsonl`, `chroma_storage/` | `output/gemini_rag_results.jsonl`       | Valuta Gemini integrato con RAG (ChromaDB), usando contesto + retrieval per predire il prossimo comando. |
| 8ï¸âƒ£  | `prompting/evaluate_ollama_topk.py`                          | `cowrie_TEST.jsonl`                | `output/ollama_topk_results.jsonl`          | Valuta modelli locali (es. CodeLlama via Ollama) in modalitÃ  Top-k senza RAG. |
| 9ï¸âƒ£  | `prompting/evaluate_ollama_rag.py`                           | `cowrie_TEST.jsonl`, `chroma_storage/` | `output/ollama_rag_results.jsonl`       | Valuta modelli locali con RAG (vector search + LLM) sulle stesse sessioni di test. |
| ğŸ”Ÿ  | `Honeypot/Vagrantfile` + `Honeypot/playbook.yml`              | â€”                                   | VM di test configurata                      | Crea lâ€™ambiente honeypot con Vagrant + Ansible (rete, pacchetti, utenti, Python, log, ecc.). |
| 1ï¸âƒ£1ï¸âƒ£ | `Honeypot/roles/fakeshell_v2/files/fakeshell.py`             | Input interattivo SSH nella VM      | `/var/log/fakeshell.json`                   | Fake shell avanzata: esegue comandi reali, mostra prompt realistico e logga ogni comando in formato JSONL. |
| 1ï¸âƒ£2ï¸âƒ£ | `Honeypot/roles/defender/files/defender2.py`                 | `/var/log/fakeshell.json`, ChromaDB | File di deception nel FS della VM           | Versione deployabile del Defender: segue il log, usa RAG+Gemini per predire i prossimi comandi e crea artefatti di deception. |
| 1ï¸âƒ£3ï¸âƒ£ | `deception/main.py` + `deception/ssh_server.py` + `session_handler.py` | Connessioni SSH reali               | Sessioni honeypot instradate verso il â€œbrainâ€ | Avvia il server SSH honeypot, accetta connessioni, gestisce le sessioni e inoltra i comandi al motore di deception. |
| 1ï¸âƒ£4ï¸âƒ£ | `deception/defender.py`                                      | Log honeypot (es. `fakeshell.json`), ChromaDB, LLM | Artefatti reali + log difese      | Defender runtime principale: legge i comandi in tempo reale, predice i prossimi passi con RAG+LLM e crea file/configurazioni esca. |
| 1ï¸âƒ£5ï¸âƒ£ | `deception/brain.py`                                         | Stato sessioni + log + predizioni   | Decisioni di deception / strategie           | Coordina a livello alto la strategia di deception (scenari, livelli di ingaggio, tipo di artefatti da generare). |
| 1ï¸âƒ£6ï¸âƒ£ | `deception/config.py`                                        | â€”                                   | Parametri di configurazione condivisi        | Centralizza porte, path, chiavi API, location del log, del DB vettoriale e degli scenari di deception. |
---

## ğŸ“Š Output di esempio

Esempio di riga in ollama_results.jsonl:
```bash
{"context": ["whoami", "uname -a"], "expected": "cat /etc/passwd", "predicted": "cat /etc/shadow", "similarity": 0.85, "match": 0, "raw_response": "cat /etc/shadow"}
```
Esempio di file summary.json:
```bash
{
  "total_new": 200,
  "exact_acc": 0.12,
  "near_matches_jaccard>=0.80": 0.35,
  "error_rate": 0.05,
  "model": "mistral:7b-instruct-q4_0",
  "generated_at": "2025-11-04T10:00:00Z"
}
```

â¸»

---

## ğŸ§  Note metodologiche

- I modelli funzionano meglio con **prompt brevi** e **in inglese**, come quelli costruiti in  
  `core_topk.py` e `core_RAG.py`.
- Le predizioni devono sempre essere pulite: estrarre **solo la prima riga valida**, usando le funzioni
  di parsing e normalizzazione in `utils.py`.
- Testare diversi valori di **context length** (`--context-len`), soprattutto con:  
  - `evaluate_ollama_topk.py`  
  - `evaluate_GEMINI_topk.py`  
  - `evaluate_ollama_RAG.py`  
  - `evaluate_GEMINI_RAG.py`
- Per valutare correttamente i modelli, utilizzare sia:
  - **Exact Match** (giÃ  implementato nel tuo codice)
  - **Confronto normalizzato** tramite `utils.normalize_for_compare()`
- Quando si usano API come Gemini, mantenere attivo **rate limit** + **sleep** (giÃ  presente nei tuoi script).
- Preferire modelli locali via **Ollama** (`codellama`, `llama3`, `mistral`, `gemma:2b`) per test massivi,
  perchÃ© gli script `evaluate_ollama_*` sono ottimizzati per esecuzioni lunghe.
- Usare dataset puliti generati da:
  - `merge_cowrie_datasets.py`
  - `analyze_and_clean.py`
  - `convert_sessions_to_finetune.py`  
  per ridurre rumore e comandi non utili allâ€™LLM.
- Per RAG, evitare di reinizializzare il DB: `VectorContextRetriever` verifica giÃ  se la collezione esiste.

---

## ğŸ”§ Possibili estensioni future

- Addestrare un modello locale tramite **fine-tuning** su `convert_sessions_to_finetune.py`
  (formato giÃ  pronto per supervised next-command prediction).
- Implementare metriche avanzate come:
  - **Top-k Accuracy**
  - **Recall@k**
  - **Confidence Distribution** dei candidati prodotti dal modello
- Integrare direttamente il motore predittivo (top-k o RAG) dentro Cowrie tramite:
  - hook sugli eventi `cowrie.command.input`  
  - API locale che richiama `evaluate_ollama_topk.py`
- Utilizzare `core_RAG.py` per creare un **Honeypot con memoria storica** degli attacchi,
  aggiornando dinamicamente ChromaDB con nuove sessioni.
- Aggiungere una pipeline di:
  - **Command Semantics Classification** per etichettare automaticamente i pattern:
    ricognizione, file exfiltration, credential harvesting, persistence, ecc.
- Costruire dashboard real-time usando i file JSONL prodotti da:
  - `evaluate_ollama_topk.py`
  - `evaluate_ollama_RAG.py`
  - `evaluate_GEMINI_RAG.py`
- Estendere il dataset includendo altri dataset pubblici (Zenodo 3759652, SIHD, HoneySELK)
  giÃ  compatibili con i tuoi script di merge e normalizzazione.

---

## ğŸ“š Riferimenti

- ğŸ **Cowrie Honeypot** â†’ https://github.com/cowrie/cowrie
- ğŸª¤ **Canarytokens** â†’ https://canarytokens.org / https://github.com/thinkst/canarytokens
- ğŸ’» **Ollama** â†’ https://ollama.com / https://github.com/ollama/ollama
- ğŸŒ **OpenRouter API** â†’ https://openrouter.ai
- ğŸ§ª **CyberLab Honeynet Dataset (Zenodo)** â†’ https://zenodo.org/records/3687527
- ğŸ¼ **PANDAcap SSH Dataset** â†’ https://zenodo.org/records/3759652
- ğŸ­ **SIHD â€“ Smart Industrial Honeypot Dataset (IEEE)** â†’ https://ieee-dataport.org/documents/sihd-smart-industrial-honeypot-dataset
- ğŸ•µï¸ **HoneySELK Cyber Attacks Dataset (IEEE)** â†’ https://ieee-dataport.org/open-access/dataset-cyber-attacks-honeyselk

### ğŸ“˜ Key Papers
- ğŸ“„ Nawrocki et al. (2016) â€” "A Survey on Honeypot Software and Data Analysis"  
- ğŸ¤– Deng et al. (2023) â€” "PentestGPT: Evaluating LLMs for Automated Penetration Testing"  
- ğŸ›¡ï¸ Alata et al. â€” "Lessons Learned from High-Interaction Honeypot Deployment"  
- ğŸ¦ Whitham â€” "Canary Tokens and Deception"  

---

## ğŸ‘¥ Autori

| | | |
|:--:|:--:|:--:|
| <a href="https://github.com/BlackRaffo70"><img src="https://github.com/BlackRaffo70.png" width="110" alt="avatar Raffaele Neri"></a> | <a href="https://github.com/melomatte"><img src="https://github.com/melomatte.png" width="110" alt="avatar Matteo Melotti"></a> | <a href="https://github.com/kikeeeee"><img src="https://github.com/kikeeeee.png" width="110" alt="avatar Enrico Borsetti"></a> |
| **Raffaele Neri**<br/>[@BlackRaffo70](https://github.com/BlackRaffo70) | **Matteo Melotti**<br/>[@melottimatteo](https://github.com/melomatte) | **Enrico Borsetti**<br/>[@enricoborsetti](https://github.com/kikeeeee) |

---

ğŸ“˜ *Progetto di ricerca:*  
**ğŸ¯ Predictive Deception â€“ LLM-based Command Anticipation in SSH Honeypots**  
UniversitÃ  di Bologna â€“ Corso di Laurea Magistrale in Ingegneria Informatica  

ğŸ‘¨â€ğŸ« *Docente referente:* **Prof. Michele Colajanni**
